# haoyu.github.io
==+== ASE'22 Review Form
==-== DO NOT CHANGE LINES THAT START WITH "==+==" OR "==*==".
==-== For further guidance, or to upload this file when you are done, go to:
==-== https://ase2022.hotcrp.com/offline

==+== =====================================================================
==+== Begin Review #458
==+== Reviewer: Tao Xie <taoxie@pku.edu.cn>

==+== Paper #458
==-== Title: Automated Repair of Auto-generated Codex Programs

==+== Review Readiness
==-== Enter "Ready" if the review is ready for others to see:

Ready

==*== Reviewer guidelines, roles, and responsibilities
==-== (hidden from authors)
==-==    In your review, make sure to consider the type of submission
==-==    (technical research paper or experience paper) and follow the
==-==    corresponding evaluation criteria from  ASE'22 reviewer
==-==    guidelines.
==-==    
==-==    Please confirm that you have read
==-==    - the ASE'22 review guide: 
==-==    https://tinyurl.com/ASE22ReviewGuide
==-==    and
==-==    - the ACM's policy on reviewer roles and responsibilities:
==-==    https://www.acm.org/publications/policies/roles-and-responsibilities
==-== Choices:
==-==    1. I confirm
==-==    2. I do not confirm
==-== Enter the number of your choice:
1
(Your choice here)

==*== Overall merit
==-== Choices:
==-==    1. Reject
==-==    2. Weak reject
==-==    3. Weak accept
==-==    4. Accept
==-==    5. Strong accept, award quality
==-== Enter the number of your choice:

(Your choice here) 4

==*== Reviewer expertise
==-== (hidden from authors)
==-== Choices:
==-==    1. I am an informed outsider
==-==    2. Knowledgeable
==-==    3. Expert
==-== Enter the number of your choice:

(Your choice here) 2

==*== Paper summary
==-==    Summarize the essence of the paper in your own words, explaining
==-==    to the fellow reviewers, program committee chairs, and the authors
==-==    what the paper is about.
==-== Markdown styling and LaTeX math supported.

This paper investigates the effectiveness of automated repair tools on auto-generated codex programs. This paper first study the faluts in the programs that produced by Codex, then it investigates the effectiveness of two existing APR tools and the Codex-e on the auto-generated programs.
This work is the first to explore the effects of existing APR tools on the buggy code generated by Codex, as well as the effects of CodeX-E. 
In addition, this paper proposes a new dataset LDMefects for automated repair.

==*== Strengths
==-==    Please provide a short bullet-point list of the paper’s key
==-==    strengths.
==-== Markdown styling and LaTeX math supported.




A detailed classification of the buggy code generated by Codex
A detailed analysis of the effectiveness of APR tools and codex-e on buggy code generated by Codex
A new dataset LDMefects for automated repair


==*== Weaknesses
==-==    Please provide a short bullet-point list of the paper’s key
==-==    weaknesses.
==-== Markdown styling and LaTeX math supported.


The experiment results may not be valid for larger models
Humaneval dataset is not used as a benchmark

==*== Comments for authors
==-==    Please provide a detailed evaluation of the paper, focusing on the
==-==    paper’s strengths, weaknesses, and suggestions for improvements.
==-== Markdown styling and LaTeX math supported.

This paper is mostly well written and does more solid experiments to invesigate the effeciveness of APR tools on the bug code generated by Codex.
First of all, I would like to appreciate the authors’ efforts and I really enjoy reading this paper.
This work first makes a detailed classification of the buggy code generated by Codex, and makes a detailed analysis of the effectiveness of APR tools and codex-e on buggy code generated by Codex.
Interesingly, this work find that when giving the faulty line number for codex-e, the effect of codex-e decreases a lot.

Recent studies have shown that with the increasing scale of the model, the syntax errors in the generated code will be less and less. Therefore, a little doubt is that the experimental conclusions in the this paper may not be valid for other large models.
At the same time, although the author explained the reasons for not using HumanEval benchmark, I still think it is necessary to use HumanEval dataset. Because HumanEval data set is completely independent of the training set of Codex, it can better investigate Codex's ability on code generation.



 

==*== Comments for PC
==-== (hidden from authors)
==-==    Confidential remarks for the program committee, which will not be
==-==    shared with the authors. Also, when updating the review, if the
==-==    change is not documented in the paper discussion, please state
==-==    here the nature of the changes you made.
==-== Markdown styling and LaTeX math supported.



==+== Scratchpad (for unsaved private notes)

==+== End Review
